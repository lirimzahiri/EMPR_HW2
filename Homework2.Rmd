---
title: "Homework 2 - Exploratory Data Analysis"
author: "Mouad Medini & Lirim Zahiri"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: true
    toc: true
---


```{r setup, include=FALSE}
# Preparation

# Initial Setup: Suppress output from code setup chunk
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(fig.width = 7, fig.height = 4)


# Load necessary libraries for data analysis and visualization
library(tidyverse)  # Main library for data manipulation and analysis (includes ggplot2, dplyr, tidyr, etc.)
library(ggridges)   # Extension for ggplot2 to create ridge plots
library(janitor)    # For data cleaning and adding utility functions like "clean_names()"
library(nycflights13)  # Dataset package for practicing data manipulation and analysis
library(readxl)  # For importing Excel files into R
```

**Disclaimer: All code chunks were excluded from the PDF due to page limitation reasons.**

**For Code please refer to the .rmd file**

# Task 1: Days with temperatures below 10°C at Newark Airport

## Objective

Determine the number of days with temperatures below 10°C at Newark Airport (EWR), grouped by month, and compare the results to expected seasonal trends.

```{r task1}
# Exploring the weather dataset
glimpse(weather)
?weather

# Filter and count for temperatures below 10°C (10° in Fahrenheit = 50) at Newark (EWR)
weather |>
  filter(origin == "EWR", temp < 50) |>
  group_by(month, date = as.Date(time_hour)) |>
  summarize(days_below_10 = n(), .groups = "drop") |>
  count(month, name = "days_below_10")
```

*(Slides used: EMPR_05_Data_Transformation_I_AS2024.pdf, EMPR_08a_EDA_AS2024.pdf)*

## Conclusion

The analysis shows that January, February, and March have the highest number of days with temperatures below 10°C, which aligns well with expectations for the winter months. As spring approaches, April and May show a noticeable decline in cold days. Interestingly, some colder days were recorded in October and November, likely due to seasonal variability. September had very few cold days, aligning with its position as a transitional month between summer and autumn.

While the results generally match expectations for Newark's weather patterns, there are small deviations. For instance, September, typically considered a summer month, surprisingly recorded a few colder days.

Overall, these findings suggest that winter in Newark tends to last longer than in Switzerland, indicating a notable difference in seasonal patterns between the two regions.

# Task 2: Analyze flights missing `dep_time`

## Objective

Determine the flights in January with missing `dep_time` and identify other variables these flights are also missing. Finally, interpret what these rows might represent.

```{r task2}
# Flights with missing `dep_time` in january
flights |>
  filter(month == 1, is.na(dep_time))
```

*(Slides used: EMPR_05_Data_Transformation_I_AS2024.pdf)*

## Results

The flights with missing `dep_time` in January are also missing data typically associated with flight operation: - `dep_delay` - `arr_time` - `arr_delay` - `air_time`

The rows with missing `dep_time` most likely represent **canceled flights**. This is supported by: 1. The absence of a departure time, suggesting the flight never left the airport. 2. Missing values for related variables such as `dep_delay`, `arr_time`, and `arr_delay`, which would only exist for flights that operated.

This conclusion aligns with the expected behavior of **canceled flights**, where operational data is not recorded.

# Task 3: Visualizing temperatures

## Objective

The goal of this task is to analyze and visualize temperature data from the `nycflights13::weather` dataset. This analysis aims to provide insights into monthly temperature patterns and variations throughout the year.

```{r task3}
# Select all variables except those between year and hour
# Add the month column from time_hour
weather_filtered <- weather |>
  select(-(year:hour)) |>
  mutate(month = month(time_hour))

# Visualization a
weather_filtered |>
  mutate(month = factor(month, labels = month.name)) |> # Convert month numbers to full names
  ggplot(aes(x = temp)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white") +
  facet_wrap(~ month, ncol = 3) +
  labs(title = "Temperature Distribution by Month", x = "Temperature (°F)", y = "Count") +
  theme_minimal()

# Ridge plot
weather_filtered |>
  mutate(month = factor(month, labels = month.name)) |> # Convert month numbers to full names
  ggplot(aes(x = temp, y = month)) +
  geom_density_ridges(scale = 0.9, fill = "skyblue", alpha = 0.7) + # Ridge plot
  labs(
    title = "Temperature Distribution by Month (Ridge Plot)",
    x = "Temperature (°F)",
    y = "Month"
  ) +
  theme_minimal()

# Aggregate data with weather_filtered
agg_data <- weather_filtered |>
  group_by(month) |>
  summarize(
    avg_temp = mean(temp, na.rm = TRUE),
    sd_temp = sd(temp, na.rm = TRUE),
    n = n()
  )

# Plot with explicit confidence interval calculation during visualization
ggplot(agg_data, aes(x = month, y = avg_temp)) +
  geom_line(size = 1, color = "blue") +                              # Line for average temperature
  geom_ribbon(
    aes(
      ymin = avg_temp - 3 * (sd_temp / sqrt(n)),                     # Lower 99% CI
      ymax = avg_temp + 3 * (sd_temp / sqrt(n))                      # Upper 99% CI
    ),
    fill = "lightblue", alpha = 0.4
  ) +
  scale_x_continuous(breaks = 1:12, labels = 1:12) +                 # Numeric month labels
  labs(
    title = "Monthly Average Temperature with 99% Confidence Intervals",
    x = "Month (Number)", y = "Average Temperature (°F)"
  ) +
  theme_minimal()
```

*(Slides used: EMPR_03_Visualization1_AS2024.pdf, EMPR_07_Data_Tidying_AS2024,*

*EMPR_05_Data_Transformation_I_AS2024, EMPR_08a_EDA_AS2024.pdf)*

## Results

* **Temperature Distribution:** Histograms show temperature distributions for each month, with colder months exhibiting lower temperature clusters. 
* **Ridge Plot:** The ridge plot illustrates the progression of temperature distributions across months, highlighting seasonal shifts.
* **Monthly Averages:** The line plot with confidence intervals reveals lower temperatures in winter, peaking in summer, and gradual transitions between seasons.

This analysis aligns with expected seasonal patterns and demonstrates the value of combining multiple visualization techniques to analyze temperature trends effectively.

# Task 4: Data cleaning from `Data_HW2.xlsx`

## Objective

The goal is to import the dataset from the sheet **"A very small sample"** in `Data_HW2.xlsx`, inspect it for issues, clean the data, and save it as a `.rds` file for further analysis. This task ensures consistency and usability of the dataset.

## Data Cleaning Issues and Solutions

1. **Column Names:** Column names were inconsistent, with spaces and special characters (e.g., `date of birth`, `cash (CHF)`). This was addressed by standardizing column names to snake_case format using `clean_names()`.

2. **Class and Gender:** The `class` and `gender` columns were not stored as factors. These columns were converted to factors using `factor()` for appropriate categorical handling. 

3. **Date of Birth:** Dates were stored in multiple formats (e.g., `YYYY-MM-DD`, Excel serial numbers). Conditional parsing was applied to handle different date formats and convert Excel serial numbers to proper dates.

4. **Height:** Height measurements were inconsistent (centimeters and meters). Numeric values were parsed, and meters were converted to centimeters for uniformity.

5. **Hair Length:** The value "Glatze" (bald) was non-numeric. "Glatze" was replaced with 0, and the column was converted to numeric.

6. **Eye Color:** Inconsistent eye color terms (mixed languages, separators) were addressed by standardizing terms to English, removing separators, and ensuring consistent formatting.

7. **Cash (CHF):** Numerical values were ensured to be properly parsed as numeric.

8. **Other Issues:** No issues were found with postcode and transport values, and no transformations were required for these columns.

```{r task4}
# Import and cleaning the dataset
# Referenced from slides: EMPR_06_Import_Export_AS2024.pdf
data <- read_excel("Data_HW2.xlsx", sheet = "A very small sample")

data |> glimpse() |>
  summary()

data_clean <- data |> 
  clean_names() |> 
  mutate(
    # Convert class and gender to factors
    class = factor(class),
    gender = factor(gender),
    
    # Correct date_of_birth: Handle standard dates and Excel serial numbers
    date_of_birth = case_when(
      str_detect(date_of_birth, "^\\d{4}-\\d{2}-\\d{2}$") ~ as.Date(date_of_birth),  # Standard YYYY-MM-DD
      str_detect(date_of_birth, "^\\d+$") ~ as.Date(as.numeric(date_of_birth), origin = "1899-12-30"),  # Excel serial
      TRUE ~ NA_Date_  # Assign NA if unparseable
    ),
    
    # Correct height: Use parse_number to extract numeric values and handle cm/m conversion
    height = case_when(
      str_detect(height, "m") ~ parse_number(height) * 100,  # Convert "1,82m" to 182
      TRUE ~ parse_number(height)  # Parse "cm" values directly
    ) / 100,  # Fix incorrect scaling
    
    # Handle hair: Replace "Glatze" with "0" and convert to numeric
    hair = parse_number(if_else(hair == "Glatze", "0", hair)),
    
    # Standardize eye_colour, keeping combinations but removing "/"
    eye_colour = str_replace_all(
      eye_colour,
      regex("braun|brown", ignore_case = TRUE), "brown"
    ) |> str_replace_all(
      regex("blau|blue", ignore_case = TRUE), "blue"
    ) |> str_replace_all(
      regex("grün|green", ignore_case = TRUE), "green"
    ) |> str_replace_all(
      regex("grau|grey|gray", ignore_case = TRUE), "grey"
    ) |> str_replace_all(
      regex("schwarz|black", ignore_case = TRUE), "black"
    ) |> str_replace_all(
      "/", " "  # Replace "/" with a space
    ) |> str_squish()  # Remove extra spaces
  ) |> 
  glimpse()

# Step 4: Save the cleaned data
write_rds(data_clean, "data_clean.rds")

# Step 5: Verify cleaned data
glimpse(data_clean)
```

*(Slides used: EMPR_05_Data_Transformation_I_AS2024)*

## Conclusion

The cleaned data ensures usability and accuracy, addressing all inconsistencies in the original dataset. The cleaned dataset is saved as `data_clean.rds` for use in future analyses.

# Task 5: Anscombe's Dataset

## Part (a): Transform the Dataset into Tidy Format

### Objective

The goal is to transform Anscombe's dataset into a tidy format with exactly three variables: `sample`, `x`, and `y`. The dataset will then be saved for future use.

### Issues and Solution

1.  **Issue**: The dataset has eight columns (`x1` to `x4`, `y1` to `y4`) instead of grouping all observations into three variables: `sample`, `x`, and `y`.
2.  **Solution**: Use `pivot_longer()` to:
    -   Gather all `x` and `y` values into a single column for each.
    -   Extract the sample number from the column names and store it in a new variable `sample`.

```{r task5_part_a}
# Transform Anscombe's dataset into tidy format

anscombe |> glimpse()

tidy_anscombe <- anscombe |>
  pivot_longer(
    cols = everything(),
    names_to = c(".value", "sample"),
    names_pattern = "(.)(.)"
  ) |>
  glimpse()

# Save the tidy dataset for future use
saveRDS(tidy_anscombe, file = "tidy_anscombe.rds")
```

*(Slides used: EMPR_07_Data_Tidying_AS2024)*

### Conclusion

The dataset was transformed into a tidy format with three variables: `sample`, `x`, and `y`. This ensures the data is consistent, easier to analyze, and adheres to tidy data principles. The tidy dataset was saved as `tidy_anscombe.rds` for further use.

## Part (b): Scatterplot with Best Fit Lines

### Objective

The objective is to create a scatterplot using the tidy Anscombe dataset, distinguishing each sample by color and shape, and including best-fit lines for `y` on `x`. Shapes are restricted to specific values (12, 10, 9, and 7) for clarity. This visualization demonstrates the value of visualizing data to reveal underlying patterns not evident in summary statistics.


```{r task5_part_b}
# Load the tidy dataset
tidy_anscombe <- readRDS("tidy_anscombe.rds")

# Scatterplot with best fit lines
ggplot(tidy_anscombe, aes(x = x, y = y, color = sample, shape = sample)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE) +
  scale_shape_manual(values = c(12, 10, 9, 7)) +
  labs(
    title = "Anscombe's Datasets: Scatterplot with Best Fit Lines",
    x = "x",
    y = "y",
    color = "Sample",
    shape = "Sample"
  ) +
  theme_minimal()
```

*(Slides used: EMPR_03_Visualization1_AS2024)*


## Part (c): Summarize the Tidy Dataset

### Objective

The objective is to summarize the tidy Anscombe dataset by calculating the means, standard deviations, and correlation coefficients for `x` and `y` grouped by sample. The results are presented in a well-formatted table using `kableExtra::kable()`.

```{r task5_part_c}
# Summarize the tidy dataset and display using kableExtra in a single pipeline
tidy_anscombe |>
  group_by(sample) |>
  summarise(
    mean_x = mean(x),
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    corr_xy = cor(x, y)
  ) |>
  kableExtra::kable(
    caption = "Summary of Anscombe's Dataset",
    col.names = c("Sample", "Mean x", "Mean y", "SD x", "SD y", "Correlation"),
    digits = 3
  )
```

*(Slides used: EMPR_05_Data_Transformation_I_AS2024, EMPR_03_Visualization1_AS2024)*

## Part (d): Facet Grid Scatterplots

### Objective

The objective is to create a set of scatterplots for each sample in Anscombe's dataset using `facet_grid()`, distinguishing points by color and shape and including lines of best fit to show linear trends. This visualization aims to highlight the differences between the samples despite identical summary statistics.


```{r task5_part_d}
# Scatterplots with facet grid per sample
ggplot(tidy_anscombe, aes(x = x, y = y)) +
  geom_point(aes(color = sample, shape = sample), size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  facet_grid(. ~ sample) +
  scale_shape_manual(values = c(12, 10, 9, 7)) +
  labs(
    title = "Anscombe's Datasets: Facet Grid Scatterplots",
    x = "x",
    y = "y",
    color = "Sample",
    shape = "Sample"
  ) +
  theme_minimal()
```

*(Slides used: EMPR_03_Visualization1_AS2024)*

## Conclusion

The facet grid scatterplots clearly show that all four samples, despite having identical statistical properties, exhibit distinct data structures and patterns. This underscores Anscombe's message: **numerical summaries alone can be misleading, and visualizations are essential to uncover meaningful insights**.

\newpage

# Collaborative Effort with Phased Approach

This project, exploring and visualizing a dataset, represents a collaborative effort between Mouad Medini and Lirim Zahiri. We leveraged the power of Git and GitHub ([https://github.com/lirimzahiri/EMPR_HW2](https://github.com/lirimzahiri/EMPR_HW2)) for efficient version control, code sharing, and seamless collaboration.

**Phased Approach for Efficiency:**

Given Mouad's prior completion of all SBD modules, we adopted a phased approach to optimize the workflow. Lirim initiated the coding process to establish a foundation, and then we both actively explored and improved the code collaboratively. This approach ensured efficient utilization of Mouad's expertise while allowing for joint learning and refinement.

**Shared Responsibilities:**

Throughout the project, we maintained a collaborative spirit:

* **Data Cleaning:** Mouad's initial data cleaning efforts provided a solid foundation for the analysis. Lirim actively reviewed and refined the cleaning process to ensure data quality.
* **Visualization:** Lirim took the lead in crafting informative visualizations, but Mouad provided valuable feedback and suggestions to enhance their clarity and effectiveness.
* **Documentation:** We collaboratively drafted the documentation, ensuring it comprehensively explained the undertaken steps, the achieved results, and the insights gained from the data exploration.

**Version Control with Git and GitHub:**

Utilizing Git for version control allowed us to track changes, revert to previous versions if necessary, and maintain a clear history of the project's development. GitHub served as a central repository, facilitating code sharing, real-time collaboration, and efficient merging of our contributions.

This collaborative approach fostered a dynamic and enriching learning experience. By working together, we were able to leverage each other's strengths, address challenges more effectively, and ultimately produce a more comprehensive and insightful data exploration project.
